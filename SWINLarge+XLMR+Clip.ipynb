{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla V100-DGXS-32GB\n",
      "Memory Allocated on GPU 1: 3.81 MB\n",
      "Memory Reserved on GPU 1: 20.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set PyTorch to use GPU 1\n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# Allocate a tensor to initialize GPU 1\n",
    "x = torch.rand(1000, 1000, device=device)\n",
    "\n",
    "# Verify the GPU being used\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(device.index)}\")\n",
    "print(f\"Memory Allocated on GPU 1: {torch.cuda.memory_allocated(device.index) / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Memory Reserved on GPU 1: {torch.cuda.memory_reserved(device.index) / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalyanreddy/miniconda3/envs/Clip_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomResizedCrop, ColorJitter, RandomRotation\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from timm import create_model\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define Image Transformations with more augmentation\n",
    "image_transform = Compose([\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    RandomRotation(20),\n",
    "    ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
    "])\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Dataset class\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, data, image_transform, tokenizer):\n",
    "        self.data = data\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.data[idx]\n",
    "        image_path = record[\"img\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        text = record[\"text\"]\n",
    "        text_tokens = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        label = torch.tensor(record[\"label\"], dtype=torch.float)\n",
    "        return image, text_tokens[\"input_ids\"].squeeze(0), text_tokens[\"attention_mask\"].squeeze(0), label\n",
    "\n",
    "# Improved CLIP-Like Fusion Module with ReLU\n",
    "class ImprovedCLIPFusion(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, fusion_dim):\n",
    "        super(ImprovedCLIPFusion, self).__init__()\n",
    "        self.image_proj = nn.Linear(image_dim, fusion_dim)\n",
    "        self.text_proj = nn.Linear(text_dim, fusion_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fusion_dim, fusion_dim)\n",
    "        self.fc2 = nn.Linear(fusion_dim, fusion_dim)\n",
    "        self.fc3 = nn.Linear(fusion_dim, 1)\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        image_proj = self.activation(self.image_proj(image_features))\n",
    "        text_proj = self.activation(self.text_proj(text_features))\n",
    "        fused_features = self.activation(image_proj + text_proj)\n",
    "        fused_features = self.dropout(fused_features)\n",
    "        fused_features = self.activation(self.fc1(fused_features))\n",
    "        fused_features = self.activation(self.fc2(fused_features))\n",
    "        output = self.fc3(fused_features)\n",
    "        return output\n",
    "\n",
    "# Define the Complete Multimodal Model with Swin-Large\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, swin_model_name, xlmr_model_name, fusion_dim):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        self.swin_model = create_model(swin_model_name, pretrained=True, num_classes=0)  # Swin-Large\n",
    "        self.xlmr_model = XLMRobertaModel.from_pretrained(xlmr_model_name)\n",
    "        self.clip_fusion = ImprovedCLIPFusion(\n",
    "            image_dim=self.swin_model.num_features,\n",
    "            text_dim=self.xlmr_model.config.hidden_size,\n",
    "            fusion_dim=fusion_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, images, texts, attention_masks):\n",
    "        image_features = self.swin_model(images)\n",
    "        text_features = self.xlmr_model(input_ids=texts, attention_mask=attention_masks).last_hidden_state[:, 0, :]  # Use [CLS] token\n",
    "        output = self.clip_fusion(image_features, text_features)\n",
    "        return output\n",
    "\n",
    "# Function to load datasets\n",
    "def load_dataset(file_path, is_jsonl=False):\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        if is_jsonl:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line.strip())\n",
    "                    data.append(record)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        else:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON file: {file_path}\")\n",
    "    return data\n",
    "\n",
    "# Training and Fine-Tuning Function\n",
    "def train_and_fine_tune_model(model, train_loader, fine_tune_loader, criterion, optimizer, scheduler, device, train_epochs, fine_tune_epochs, save_path=None):\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(train_epochs + fine_tune_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Gradual unfreezing of pre-trained models\n",
    "        if epoch < 5:\n",
    "            for param in model.swin_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.xlmr_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in model.swin_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in model.xlmr_model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        loader = train_loader if epoch < train_epochs else fine_tune_loader\n",
    "        phase = \"Training\" if epoch < train_epochs else \"Fine-Tuning\"\n",
    "\n",
    "        for images, texts, attention_masks, labels in loader:\n",
    "            images, texts, attention_masks, labels = (\n",
    "                images.to(device),\n",
    "                texts.to(device),\n",
    "                attention_masks.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images, texts, attention_masks).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"{phase} Epoch [{epoch + 1}/{train_epochs + fine_tune_epochs}], Loss: {epoch_loss / len(loader):.4f}\")\n",
    "\n",
    "    if save_path:\n",
    "        save_model(model, save_path)\n",
    "\n",
    "# Save and Load Functions\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    if os.path.exists(path):\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found at {path}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, texts, attention_masks, labels in dataloader:\n",
    "            images, texts, attention_masks, labels = (\n",
    "                images.to(device),\n",
    "                texts.to(device),\n",
    "                attention_masks.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            outputs = model(images, texts, attention_masks).squeeze()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Paths\n",
    "train_file = \"/home/kalyanreddy/Datasets/English/Train.jsonl\"\n",
    "fine_tune_file = \"/home/kalyanreddy/Datasets/Finetune.json\"\n",
    "test_file = \"/home/kalyanreddy/Datasets/MultiTest.json\"\n",
    "model_save_path = \"multimodal_clip_model_swin_large.pth\"\n",
    "\n",
    "# DataLoader parameters\n",
    "batch_size = 64\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_dataset(train_file, is_jsonl=True)\n",
    "fine_tune_data = load_dataset(fine_tune_file, is_jsonl=False)\n",
    "test_data = load_dataset(test_file, is_jsonl=False)\n",
    "\n",
    "train_loader = DataLoader(MultimodalDataset(train_data, image_transform, xlmr_tokenizer), batch_size=batch_size, shuffle=True)\n",
    "fine_tune_loader = DataLoader(MultimodalDataset(fine_tune_data, image_transform, xlmr_tokenizer), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(MultimodalDataset(test_data, image_transform, xlmr_tokenizer), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model with Swin-Large\n",
    "multimodal_model = MultimodalClassifier(\n",
    "    swin_model_name=\"swin_large_patch4_window7_224\",  # Swin-Large\n",
    "    xlmr_model_name=\"xlm-roberta-base\",\n",
    "    fusion_dim=1024  # Increased fusion dimension\n",
    ").to(device)\n",
    "\n",
    "# Loss, Optimizer, and Scheduler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    [{'params': multimodal_model.clip_fusion.parameters(), 'lr': 1e-4},\n",
    "     {'params': multimodal_model.swin_model.parameters(), 'lr': 1e-5},\n",
    "     {'params': multimodal_model.xlmr_model.parameters(), 'lr': 1e-5}],\n",
    "    weight_decay=1e-2\n",
    ")\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/40], Loss: 0.6580\n",
      "Training Epoch [2/40], Loss: 0.6530\n",
      "Training Epoch [3/40], Loss: 0.6446\n",
      "Training Epoch [4/40], Loss: 0.6427\n",
      "Training Epoch [5/40], Loss: 0.6334\n",
      "Training Epoch [6/40], Loss: 0.6304\n",
      "Training Epoch [7/40], Loss: 0.6277\n",
      "Training Epoch [8/40], Loss: 0.6230\n",
      "Training Epoch [9/40], Loss: 0.6186\n",
      "Training Epoch [10/40], Loss: 0.6190\n",
      "Training Epoch [11/40], Loss: 0.6056\n",
      "Training Epoch [12/40], Loss: 0.5672\n",
      "Training Epoch [13/40], Loss: 0.5672\n",
      "Training Epoch [14/40], Loss: 0.5427\n",
      "Training Epoch [15/40], Loss: 0.5217\n",
      "Training Epoch [16/40], Loss: 0.5023\n",
      "Training Epoch [17/40], Loss: 0.4885\n",
      "Training Epoch [18/40], Loss: 0.4855\n",
      "Training Epoch [19/40], Loss: 0.4581\n",
      "Training Epoch [20/40], Loss: 0.4402\n",
      "Training Epoch [21/40], Loss: 0.4104\n",
      "Training Epoch [22/40], Loss: 0.3841\n",
      "Training Epoch [23/40], Loss: 0.3605\n",
      "Training Epoch [24/40], Loss: 0.3401\n",
      "Training Epoch [25/40], Loss: 0.3441\n",
      "Fine-Tuning Epoch [26/40], Loss: 0.9293\n",
      "Fine-Tuning Epoch [27/40], Loss: 0.7176\n",
      "Fine-Tuning Epoch [28/40], Loss: 0.5772\n",
      "Fine-Tuning Epoch [29/40], Loss: 0.5405\n",
      "Fine-Tuning Epoch [30/40], Loss: 0.4877\n",
      "Fine-Tuning Epoch [31/40], Loss: 0.4609\n",
      "Fine-Tuning Epoch [32/40], Loss: 0.4199\n",
      "Fine-Tuning Epoch [33/40], Loss: 0.3769\n",
      "Fine-Tuning Epoch [34/40], Loss: 0.3669\n",
      "Fine-Tuning Epoch [35/40], Loss: 0.3445\n",
      "Fine-Tuning Epoch [36/40], Loss: 0.3217\n",
      "Fine-Tuning Epoch [37/40], Loss: 0.2670\n",
      "Fine-Tuning Epoch [38/40], Loss: 0.2560\n",
      "Fine-Tuning Epoch [39/40], Loss: 0.1983\n",
      "Fine-Tuning Epoch [40/40], Loss: 0.1858\n",
      "Model saved to multimodal_clip_model_swin_large.pth\n"
     ]
    }
   ],
   "source": [
    "# Train and Fine-Tune Model\n",
    "train_and_fine_tune_model(\n",
    "    multimodal_model,\n",
    "    train_loader,\n",
    "    fine_tune_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    train_epochs=25,\n",
    "    fine_tune_epochs=15,\n",
    "    save_path=model_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.99%\n",
      "Precision: 0.70, Recall: 0.77, F1-Score: 0.73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7199197860962567,\n",
       " 0.7017114914425427,\n",
       " 0.7663551401869159,\n",
       " 0.7326100829610721)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(multimodal_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Clip_env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
